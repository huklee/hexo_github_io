---
title: 1st Deepcon.info Day 1 @ Seoul
date: 2017-02-16 10:47:11
tags:
- Deep learning
- Machine learning
categories:
- Conference
banner:
- http://static.boredpanda.com/blog/wp-content/uploads/2015/08/computer-deep-learning-algorithm-painting-masters-12.jpg
---
# 제1회 딥러닝 컨퍼런스 2017 1일차
주최 : 서형석 대표 @ 딥러닝그룹

# 1. 베이지안 딥러닝 - 김용대 : 서울대학교 통계학과
[slides](https://drive.google.com/file/d/0B8v4MKOWQrJAa3J1bXRFTERWbEU/view)
## Intro
Bayesian에 전반적인 내용을 다룰 예정. 끝 부분에서 베이지안 딥러닝에 대해서 언급하고 발표를 마칠 예정임.

- 사후분포Posterior distribution는 우도함수Likelihood function 과 사전분포Prior distribution의 곱에 비례한다.
  ![](http://www.saedsayad.com/images/Bayes_rule.png)

- 시간에 따라서 분포가 변하는 상황에 대해서 bayesian 접근은 꽤나 좋은 접근방식이 될 수 있다.

- 장점 : prediction의 정확도가 매우 높은 편 / Uncertainty quantization  
  - 불확실성에 대한 확률을 계산하기 좋다

- 단점 : computation is hard / Need full observation of distribution


## Bayesian computation
크게 세 가지로 나누어진다. 손으로 구하기 / monte-carlo 사용하기 / approximation 사용하기 (나머지 3개)
 * Analytical approaches with conjugate priors
 * Monte-Carlo method
 * Variational Inference
 * Assumed Density Filtering
 * Expectation Propagation
  * assume density의 확장판

## Deep learning with bayesian 모델
시작한 지 얼마 되지 않아서 아직 가시적인 연구결과에 대해서는 적용된 것이 없다.
### 딥러닝의 여러 가지 아키텍쳐
- deep neural network
![](http://www.rsipvision.com/wp-content/uploads/2015/04/Slide5.png)
- deep belief network / deep boltzmann machine ![](http://prog3.com/sbdm/img.my/uploads/201304/10/1365561611_3496.jpg)
- stack encoder
![](https://wikidocs.net/images/page/3413/sDA.png)

실제로는 deep neural network만 사용되고 나머지는 참고로만 알아두면 된다. 주로 CNN, RNN이 사용된다는 것마 알아두면, 상관없다.

### Deep latent gaussian model (DLGM)
### Probabilistic Neural Network Models
 - 둘 다 모두 수학적으로 엄청나게 어려운 계산이 들어가는 부분이라 이해하고 사용하기가 쉽지 않다. approximation을 하는 방법이 다양하게 가능하다.


# 2. deep generative model 관련 최근 연구들 - [신진우 : KAIST 전기전자](http://nia.kaist.ac.kr/shin.html)
## Recent trends
- classification : CNN
- natural language process : RNN

### Generative deep learning
- Application : image regeneration & recommendation

> example
  - generate a image for the style of artist
  ![](http://static.boredpanda.com/blog/wp-content/uploads/2015/08/computer-deep-learning-algorithm-painting-masters-12.jpg)
  - making a 복원된 image
  ![](https://tryolabs.com/images/blog/post-images/2016-12-06-major-advancements-in-deep-learning-2016/superresolution.png)
  - image compression

#### Part I : graphical model approach
  - Markov random field, restricted Boltzmann machine

Approach
  - Gibbs distribution
 ![](http://file.scirp.org/Html/6-2590028/42617e8f-bdb7-4bc9-b5dc-7c68091d670f.jpg)

  - MRF(Markov Random Field) / RBM(Restricted Boltzmann Machines) / DBM(Depp Boltzmann Machines)
  : two-state markov state
  ![](/images/deep_learning_example_horse.jpg)

#### Part II : Neural network approach
  - de-noising auto-encoder, variational auto-generator

  - 가장 `Hot한 토픽 논문`

  [Generatvie adversarial Network (GAN) [Goodfellow et al.2014]](https://arxiv.org/pdf/1406.2661.pdf)

  [Deep convolutional GAN [radford et al. 2015]](https://arxiv.org/pdf/1511.06434.pdf)


### Game and decision making
  - deep reinforcement learning

### Research in agorithmic intelligence lab
 1. image compression via neural networks
   - jpeg을 뛰어 넘는 그림 압축 알고리즘 만들어내기 : rate 120
   - 굉장히 의미 있는 가시적 결과가 나오고 있음
   1.  NP-Hard 문제 도전하기 : TSP 문제

### Lab 서버 운영
 - Lab에서 사용하고 있는 GPU 서버 50대 운영 : Titan X
 - 유명한 해외 Lab은 100대 정도는 돌리는 경우도 많다고 함

# 3. Building Deep Learning - 김인중 : 한동대학교

## A Review of a Neural Network algorithms
vector형태로 표현할 수 있는 모든 데이터는 Neural Network의 입력, 혹은 출력으로 사용될 수 있다.

## Implmentation of Neural Network
[C++로 구현한 HGU Neural Network](https://github.com/callee2006/HGUNeuralNetworks)

## Deep learning algorithms on GPU
 - 특징
  - GPU 특성상 ALU가 수백개에서 수천개를 내부에 가지고 있다.
 - 성능
  - 일반적으로는 딥 러닝 알고리즘을 적용할 때 CPU에 비해서 이미지 인식은 60배 정도 빠르며, training을 하는 데 있어서는 20배 정도 성능의 차이가 난다.
 - 주의할 점
  - parallel하게 연산을 처리할 수 있는 상황이 아니라면 오히려 CPU보다 연산이 느릴 수도 있는 상황이 존재할 수 있으니 주의해야 한다.

## Neural Networks Demonstration on XOR
 - 직접 소스코드를 짜 보면, 많으 부분 deep learning에 대한 이해도가 높아진다.
 - learning rate은 왠만하면 작게 하는 게 좋음 (0.001 정도)

## Generated Neural Networks
 - ex> autoencoder, restricted Boltzmann Machines
 - generated같은 경우는 원래 자신의 형태를 다시 만들어야 하는 constraint를 가지고 있다. 따라서 hidden layer가 일반적인 NN에 비해서 많이 가지고 있게 된다.
 - generated가 일반적인 NN 보다는 훨씬 어려운 문제를 다루고 있다고 볼 수 있으며, 응용할 수 있는 분야도 많다고 볼 수 있다.

## Restricted Boltzmann Machines : RBM
 - 역시 양방향 네트워크의 특성을 가지고 있음.
 - energy function + sigmoid function으로 표현됨
 - RBM input / output
 ![](https://deeplearning4j.org/img/multiple_inputs_RBM.png)
 - RBM probability functions
 ![](https://image.slidesharecdn.com/p04restrictedboltzmannmachinescvpr2012deeplearningmethodsforvision-120822081652-phpapp02/95/p04-restricted-boltzmann-machines-cvpr2012-deep-learning-methods-for-vision-14-728.jpg?cb=1345623580)

## RNN : Recurrent Neural Network
### Main features
 - RNNs are Turing-complete
 - sequence to sequence mapping
 ![](http://karpathy.github.io/assets/rnn/diags.jpeg)
 - best approach to following problems
   - Speech recognition
     - skype translation (error rate goes 45% -> 17%:LSTM)
    ![Skype Translation](https://blogs.skype.com/wp-content/uploads/2014/12/translatory-ui1.png)
   - handwriting recognition
   - machine translation

### problems in RNN
 - vanishing gradient problems
   - long term dependency의 문제가 있음
   ![](http://eric-yuan.me/wp-content/uploads/2015/06/1.jpg)
   - But, LSTM approach solved this problem
   ![](https://deeplearning4j.org/img/gates_lstm.png)
   - 더 자세한 설명은 다음 참고 [Understanding LSTM RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Practical Advices
  - deep learning을 제대로 이해하는 데 있어서 실제로 알고리즘의 내부를 실제로 코딩을 하는 것을 크게 추천한다.
  - 하지만, 굉장히 복잡한 작업이며, 특히나 CNN, RNN 등은 그 복잡도가 엄청 어렵다.
  - Theory를 완전히 이해해야만 수식을 하나하나 되집어 가면서 봐야만 한다.
  - 특히 GPU는 fault tolerant한 특성을 가지고 있기 떄문에, 디버깅이 매우 힘든 프로그래밍 중 하나이다. 따라서, 디버깅 툴인 trace 등을 통해서 제대로 잡히지 않는 경우가 허다하다.
  - 디버거를 통하지 않고, trace & watch만으로는 힘든 부분이 있어서, 머리로 모든 알고리즘의 로직을 완성하고, 디버깅을 수행하기를 추천함.
  - CPU 알고리즘을 먼저 구현하고, 그 다음에 GPU 알고리즘을 구현하는 것을 추천함.

## Q & A

## Useful Links
[모두를 위한 머신러닝/딥러닝 - 김성훈 교수](https://hunkim.github.io/ml/)
[Deep Learning Research Blog by Andrej Karpathy](http://karpathy.github.io/)
[Understanding LSTM RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[Back-propagation algorithm for MLP, CNN, RNN by 김수형 전남대 교수](http://pr.jnu.ac.kr/shkim/dlc.pdf)
